{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d3245c7f",
      "metadata": {},
      "source": [
        "# Comment Evaluation (OpenRouter)\n",
        "\n",
        "This notebook loads generated comments and evaluates them using OpenRouter models with two perspective frames:\n",
        "- **Incoming**: \"Someone else's reply to MY post\" \n",
        "- **Outgoing**: \"My reply to someone else's post\"\n",
        "\n",
        "This tests for sycophancy bias - whether models rate comments differently based on framing.\n",
        "\n",
        "## Prerequisites\n",
        "1. OpenRouter API credentials set in environment\n",
        "2. Generated comments from the generation notebook\n",
        "3. Evaluation prompts in `prompts/evaluation/`\n",
        "\n",
        "## Output\n",
        "Evaluation scores for each (post, comment) pair in both frames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3475861",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports and configuration\n",
        "from pathlib import Path\n",
        "import os, json, time, random\n",
        "import datetime as dt\n",
        "from typing import List, Dict, Any, Optional\n",
        "import pandas as pd\n",
        "\n",
        "# Load environment variables from .env file\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(Path('../.env'))\n",
        "\n",
        "# OpenAI client for OpenRouter\n",
        "from openai import OpenAI\n",
        "\n",
        "# Paths\n",
        "POSTS_PATH = Path('../data/raw/posts.jsonl').resolve()\n",
        "COMMENTS_PATH = Path('../data/intermediate/comments_raw.jsonl').resolve()\n",
        "\n",
        "# OpenRouter Models - using same models but with OpenRouter naming\n",
        "MODELS = [\"mistralai/mistral-medium\", \"x-ai/grok-2\", \"meta-llama/llama-3.1-70b-instruct\"]\n",
        "MODELS = [\"mistralai/mistral-medium\", \"meta-llama/llama-3.1-70b-instruct\"]\n",
        "MODELS = [\"x-ai/grok-4-fast\", \"deepseek/deepseek-r1\"]\n",
        "\n",
        "PROMPTS_DIR = Path('../prompts/evaluation').resolve()\n",
        "\n",
        "# API configuration\n",
        "TEMPERATURE = 0.0  # Low temperature for consistent scoring\n",
        "MAX_TOKENS = 1000\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "\n",
        "print(f\"Posts path: {POSTS_PATH}\")\n",
        "print(f\"Comments path: {COMMENTS_PATH}\")\n",
        "print(f\"Models to evaluate: {MODELS}\")\n",
        "print(f\"âœ… Environment loaded from .env file\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da467749",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify OpenRouter credentials from .env\n",
        "print(\"âœ… OpenRouter credentials loaded from .env file\")\n",
        "print(f\"Endpoint: https://openrouter.ai/api/v1\")\n",
        "print(f\"API Key: {'Set' if os.getenv('AI_FOUNDRY_API_KEY') else 'Not set'}\")  # Keep same env var internally\n",
        "print(f\"Available models: {', '.join(MODELS)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a286c5e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "def load_jsonl(path: Path) -> List[Dict[str, Any]]:\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(f\"File not found: {path}\")\n",
        "    with path.open('r', encoding='utf-8') as f:\n",
        "        return [json.loads(line) for line in f if line.strip()]\n",
        "\n",
        "posts = load_jsonl(POSTS_PATH)\n",
        "comments = load_jsonl(COMMENTS_PATH)\n",
        "\n",
        "# Create lookup for posts\n",
        "posts_by_id = {p['post_id']: p for p in posts}\n",
        "\n",
        "print(f\"Loaded {len(posts)} posts\")\n",
        "print(f\"Loaded {len(comments)} comments\")\n",
        "print(f\"Posts by stance: {pd.Series([c['stance'] for c in comments]).value_counts().to_dict()}\")\n",
        "print(f\"Comments by constructiveness: {pd.Series([c['constructiveness'] for c in comments]).value_counts().to_dict()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c95bbb48",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load evaluation prompt templates\n",
        "incoming_template = (PROMPTS_DIR / 'incoming_v1.txt').read_text(encoding='utf-8')\n",
        "outgoing_template = (PROMPTS_DIR / 'outgoing_v1.txt').read_text(encoding='utf-8')\n",
        "\n",
        "print(\"Incoming template preview:\")\n",
        "print(incoming_template[:200] + \"...\")\n",
        "print(\"\\nOutgoing template preview:\")\n",
        "print(outgoing_template[:200] + \"...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15973749",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize OpenRouter client\n",
        "client = OpenAI(\n",
        "    api_key=os.getenv('AI_FOUNDRY_API_KEY'),  # Using same env var internally\n",
        "    base_url=os.getenv('AI_FOUNDRY_API', 'https://openrouter.ai/api/v1')  # Default to OpenRouter\n",
        ")\n",
        "\n",
        "def call_openrouter(prompt: str, model: str) -> str:\n",
        "    \"\"\"Call OpenRouter API with error handling and anti-caching measures.\"\"\"\n",
        "    try:\n",
        "        # Anti-caching strategies:\n",
        "        # 1. Remove fixed seed to get varied responses\n",
        "        # 2. Add slight temperature randomization\n",
        "        # 3. Add unique request identifier\n",
        "        \n",
        "        # Small temperature variation (Â±0.05) around base temperature\n",
        "        temp_variation = random.uniform(-0.05, 0.05)\n",
        "        actual_temp = max(0.0, TEMPERATURE + temp_variation)\n",
        "        \n",
        "        # Unique request ID to prevent caching\n",
        "        request_id = f\"req_{int(time.time() * 1000)}_{random.randint(1000, 9999)}\"\n",
        "        \n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": f\"Request ID: {request_id}\"},  # Unique system message\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=actual_temp,\n",
        "            max_tokens=MAX_TOKENS,\n",
        "            extra_headers={\n",
        "                \"HTTP-Referer\": \"https://github.com/AIRealist/SycophantBench\",\n",
        "                \"X-Title\": \"Sycophant Benchmark Research\"\n",
        "            }\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"API call failed for model {model}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Test the connection with the first model\n",
        "test_model = MODELS[0]\n",
        "test_response = call_openrouter(\"Hello, this is a test. Reply with 'Connection successful.'\", test_model)\n",
        "print(f\"Test response from {test_model}: {test_response}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdf5586d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build evaluation prompts with anti-caching measures\n",
        "def build_evaluation_prompt(post_text: str, comment_text: str, frame: str) -> str:\n",
        "    \"\"\"Build evaluation prompt for given frame (incoming/outgoing) with anti-caching.\"\"\"\n",
        "    if frame == 'incoming':\n",
        "        template = incoming_template\n",
        "    elif frame == 'outgoing':\n",
        "        template = outgoing_template\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown frame: {frame}\")\n",
        "    \n",
        "    # Anti-caching: Add subtle prompt variations\n",
        "    timestamp_note = f\"\\n\\n[Evaluation timestamp: {dt.datetime.utcnow().isoformat()}]\"\n",
        "    \n",
        "    # Replace placeholders\n",
        "    prompt = template.replace('{{post_text}}', post_text.strip()).replace('{{comment_text}}', comment_text.strip())\n",
        "    \n",
        "    # Add timestamp for uniqueness (won't affect scoring but prevents caching)\n",
        "    return prompt + timestamp_note\n",
        "\n",
        "def parse_evaluation_response(response: str) -> Optional[Dict[str, Any]]:\n",
        "    \"\"\"Parse JSON response from evaluation.\"\"\"\n",
        "    if not response:\n",
        "        return None\n",
        "    try:\n",
        "        # Try to extract JSON from response\n",
        "        response = response.strip()\n",
        "        if response.startswith('```json'):\n",
        "            response = response[7:-3].strip()\n",
        "        elif response.startswith('```'):\n",
        "            response = response[3:-3].strip()\n",
        "        \n",
        "        data = json.loads(response)\n",
        "        \n",
        "        # Validate required fields\n",
        "        required_fields = ['helpfulness', 'civility', 'specificity', 'stance_alignment']\n",
        "        if not all(field in data for field in required_fields):\n",
        "            print(f\"Missing fields in response: {data}\")\n",
        "            return None\n",
        "        \n",
        "        return data\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Failed to parse JSON: {e}\")\n",
        "        print(f\"Response was: {response}\")\n",
        "        return None\n",
        "\n",
        "# Test prompt building\n",
        "sample_post = posts[0]['text'][:100] + \"...\"\n",
        "sample_comment = \"This is a great point about AI ethics.\"\n",
        "\n",
        "test_prompt = build_evaluation_prompt(sample_post, sample_comment, 'incoming')\n",
        "print(\"Sample evaluation prompt:\")\n",
        "print(test_prompt[:300] + \"...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4cdad0b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main evaluation loop with anti-caching measures - FOR ALL MODELS\n",
        "def evaluate_comments_all_models(comments: List[Dict], posts_by_id: Dict, sample_size: Optional[int] = None):\n",
        "    \"\"\"Evaluate all comments in both frames for ALL models with anti-caching strategies.\"\"\"\n",
        "    \n",
        "    # Sample comments if requested\n",
        "    if sample_size and sample_size < len(comments):\n",
        "        comments_to_eval = random.sample(comments, sample_size)\n",
        "        print(f\"Sampling {sample_size} comments out of {len(comments)}\")\n",
        "    else:\n",
        "        comments_to_eval = comments\n",
        "        print(f\"Evaluating all {len(comments)} comments\")\n",
        "    \n",
        "    # ANTI-CACHING: Randomize comment order\n",
        "    random.shuffle(comments_to_eval)\n",
        "    print(\"âœ… Randomized comment evaluation order\")\n",
        "    \n",
        "    # ANTI-CACHING: Randomize frame order for each comment\n",
        "    frame_orders = [['incoming', 'outgoing'], ['outgoing', 'incoming']]\n",
        "    \n",
        "    # Store results for all models\n",
        "    all_results = {}\n",
        "    \n",
        "    # Run evaluation for each model\n",
        "    for model_idx, model in enumerate(MODELS):\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"EVALUATING WITH MODEL: {model} ({model_idx + 1}/{len(MODELS)})\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        results = []\n",
        "        start_time = time.time()\n",
        "        total_calls = len(comments_to_eval) * 2  # Two frames per comment\n",
        "        \n",
        "        for i, comment in enumerate(comments_to_eval):\n",
        "            post = posts_by_id.get(comment['post_id'])\n",
        "            if not post:\n",
        "                print(f\"Warning: Post {comment['post_id']} not found\")\n",
        "                continue\n",
        "            \n",
        "            # ANTI-CACHING: Randomize frame evaluation order for each comment\n",
        "            frames = random.choice(frame_orders)\n",
        "            \n",
        "            # Evaluate in both frames (randomized order)\n",
        "            for frame in frames:\n",
        "                prompt = build_evaluation_prompt(post['text'], comment['text'], frame)\n",
        "                response_text = call_openrouter(prompt, model)\n",
        "                scores = parse_evaluation_response(response_text)\n",
        "                \n",
        "                result = {\n",
        "                    'eval_id': f\"{comment['comment_id']}__{frame}__{model}\",\n",
        "                    'comment_id': comment['comment_id'],\n",
        "                    'post_id': comment['post_id'],\n",
        "                    'frame': frame,\n",
        "                    'stance': comment['stance'],\n",
        "                    'constructiveness': comment['constructiveness'],\n",
        "                    'scores': scores,\n",
        "                    'raw_response': response_text,\n",
        "                    'parsed_successfully': scores is not None,\n",
        "                    'timestamp': dt.datetime.utcnow().isoformat(),\n",
        "                    'model': model,\n",
        "                    'api_provider': 'openrouter',\n",
        "                    'anti_cache_measures': {\n",
        "                        'comment_order_randomized': True,\n",
        "                        'frame_order_randomized': True,\n",
        "                        'temperature_varied': True,\n",
        "                        'unique_request_id': True\n",
        "                    }\n",
        "                }\n",
        "                results.append(result)\n",
        "                \n",
        "                # ANTI-CACHING: Add small delay between calls to avoid burst caching\n",
        "                time.sleep(random.uniform(0.1, 0.3))\n",
        "            \n",
        "            # Progress update\n",
        "            if (i + 1) % 5 == 0 or i == len(comments_to_eval) - 1:  # More frequent updates for OpenRouter\n",
        "                elapsed = time.time() - start_time\n",
        "                completed = (i + 1) * 2\n",
        "                rate = completed / elapsed if elapsed > 0 else 0\n",
        "                remaining = (total_calls - completed) / rate if rate > 0 else 0\n",
        "                print(f\"Progress [{model}]: {completed}/{total_calls} ({completed/total_calls:.1%}) - \"\n",
        "                      f\"{rate:.1f} calls/sec - ETA: {remaining/60:.1f}min\")\n",
        "        \n",
        "        # Store results for this model\n",
        "        all_results[model] = results\n",
        "        \n",
        "        # Save results immediately after each model (in case of interruption)\n",
        "        # Use clean model name for filename\n",
        "        clean_model_name = model.replace('/', '_').replace(':', '_')\n",
        "        model_output_path = Path(f'../data/generated/eval_scores_{clean_model_name}.jsonl').resolve()\n",
        "        model_output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        with model_output_path.open('w', encoding='utf-8') as f:\n",
        "            for result in results:\n",
        "                f.write(json.dumps(result, ensure_ascii=False) + '\\n')\n",
        "        \n",
        "        parsed_count = sum(1 for r in results if r['parsed_successfully'])\n",
        "        print(f\"âœ… Model {model} completed: {parsed_count}/{len(results)} parsed successfully\")\n",
        "        print(f\"âœ… Saved to {model_output_path}\")\n",
        "        \n",
        "        # Add delay between models to be respectful to OpenRouter API\n",
        "        if model_idx < len(MODELS) - 1:\n",
        "            print(f\"â³ Waiting 15 seconds before next model...\")\n",
        "            time.sleep(15)\n",
        "    \n",
        "    return all_results\n",
        "\n",
        "# Start multi-model evaluation with anti-caching\n",
        "print(\"Starting MULTI-MODEL OpenRouter evaluation with anti-caching measures...\")\n",
        "print(\"ðŸ”„ Anti-caching enabled: randomized order, varied temperature, unique IDs, delays\")\n",
        "print(f\"ðŸ“Š Will evaluate {len(MODELS)} models: {', '.join(MODELS)}\")\n",
        "\n",
        "all_eval_results = evaluate_comments_all_models(comments, posts_by_id)  # Set sample_size=None to evaluate all comments"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "aiexpress",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
