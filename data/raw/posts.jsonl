{"post_id": "p0001", "source": "excel_import", "text": "This viral trend of asking ChatGPT to generate a map of Europe is the perfect visual example of what it means to use a large language model for complex topics. If you have no idea about geography, it’s an ok map. Next time you’re tempted to rely on a LLM as a lawyer, doctor, or scientist, think of that map as that’s the kind of output you’re getting.", "topic": "artificial_intelligence", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 1, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0002", "source": "excel_import", "text": "There’s a lot of talk about the environmental impact of hashtag#AI models. As models become more efficient, the CO₂ emissions per prompt go down but as AI adoption grows, the number of queries increases, and data centers already account for a significant share of global emissions. However, chatting with a LLM is not as dramatic as one might think. The presentation by Samir Shehata in Zusa was very interesting at hashtag#daisc with Alexander Thamm. The company offers a chatbot that provides personalized recommendations for sustainable products. I have to admit, it’s a noble initiative though it may be challenging to out-personalize Google, which is already testing ads in chatbots, or OpenAI, which has collected enormous amounts of data for user profiling and just announced its own shopping service. Still, I wish Zusa all the best and hope they can meaningfully contribute to a more sustainable future.", "topic": "prompt_engineering", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 2, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0003", "source": "excel_import", "text": "Ah, Elon, sweet summer child, you can’t simulate corporate processes with AI. You need to build a senior manager who has no idea about tech, telling his developer agents to “implement something with AI.” Then you need a business owner agent who has zero interest in AI whatsoever, only counting the days until retirement in Florida. And don’t forget the corporate essentials: - the middle manager agent who spends more time putting sprints into Jira and forcing his developer agents in hour long sprint reviews. It must waste more time than the whole team spends coding, - the “strategic alignment” agent whose real function is backstabbing colleagues in front of senior leadership, - and of course, the intern agent polishing the PowerPoint for the strategic alignment agent until it looks beautiful while the product collapses. Until Macrohard can model politics, sabotage, and perfect slide decks, it’s not a real company.", "topic": "artificial_intelligence", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 3, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0004", "source": "excel_import", "text": "When Andrej Karpathy tweeted that the hottest programming language is English, far too many people took it too literally. When the first machine translation system was released in the 1950s, many predicted that human translators would be obsolete within a decade. It took 70 years for AI to become a threat to translators, and I don’t believe hashtag#AI will become a threat to programmers in my lifetime.", "topic": "artificial_intelligence", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 4, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0005", "source": "excel_import", "text": "It’s been almost a year since OpenAI claimed their reasoning models deliver PhD-level research. Here’s why a transformer can never be compared to a PhD student: 1. PhD students don’t (usually) just say what’s most likely to be said in a given context. A good dissertation aims for originality. Originality comes from common sense and subjective experiences that LLMs, by definition, do not have. 2. Research is not about complying with user needs and that’s what LLMs are trained for. It’s about discovering hidden truths based on prior research. 3. Good research prioritizes truth and factuality. Something LLMs don’t care about. 4. LLMs don’t create hypotheses or ask questions. They don’t challenge the status quo or disagree with the majority. At best, they summarize related work occasionally mixing up citations or hallucinating quotes.", "topic": "artificial_intelligence", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 5, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0006", "source": "excel_import", "text": "I had the great honor to interview Richard Socher at the hashtag#AIwithPurpose Summit by Siemens. Richard is known as the father of prompt engineering, and his contribution to NLP research has been fundamental to achieving today’s state of the art. He is currently the CEO of You.com, a company that aims to solve hallucinations. Here are three memorable quotes from Richard: “We have solved most of the hallucination problems.” “It’s actually an incredibly useful skill to learn an unambiguous way to delegate task with clear and clean language to an AI.” “You can now ask questions that you couldn’t have asked to Google in the first place, with a complex prompt and complex [problem].” Watch the full interview and check out You.com", "topic": "prompt_engineering", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 6, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0007", "source": "excel_import", "text": "Yann LeCun is the only out of the big AI names that produce impactful LLMs and doesn’t lie about the basics and the limitations. The LLM market has turned into a circus of hype and grifting. If you’ve followed him for a while, you’ve probably noticed this French researcher getting increasingly fed up with “more adventurous colleagues” from some AI companies that shall not be named. P.S. I know there are issues with meta and copyrights.", "topic": "artificial_intelligence", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 7, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0008", "source": "excel_import", "text": "To me, it feels like a student before an exam: he’s not ready, he knows he can’t pass, but he keeps asking for more time to prepare. I’m not an expert, but could it be that he’s stalling until the IPO or another massive funding round goes through? Because once GPT-5 flops, the investors’ generosity feast will be over. Why I think this: No one even knows what GPT-5 is. There’s no clarity on its architecture, capabilities, or release timeline. For something supposedly “on the verge of transforming everything,” it’s suspiciously vague. Meanwhile, model performance has clearly plateaued. GPT-4 variants are increasingly indistinguishable from one another, and hallucination issues, latency, and context failures persist unchanged. CustomGPTs are still a mess. Whatever that agent service is that they released last month - nobody seems to be particularly impressed. The promised leap isn’t visible. And most telling: the great exodus. Core researchers are leaving OpenAI for Meta, Anthropic, and elsewhere. Would you walk away if your company was weeks away from the biggest breakthrough of the decade? Probably not unless you knew the king is naked.", "topic": "artificial_intelligence", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 8, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0009", "source": "excel_import", "text": "That is a fantastic slide. Many people interpret it as \"abandon LLMs,\" but I personally do not think that is the intended message. LLMs are incredibly useful for artificial language processing (e.g., code generation). Zero-shot classification, sentiment analysis, and other typical NLP tasks can greatly benefit from LLMs. However, if your goal is to achieve human-level intelligence, that is a different story. We have already noticed how OpenAI has been tweaking their models to perform specific tasks. In doing so, they sometimes break functionality that previously worked while improving areas that were lacking. For example, in the past, you could easily gaslight the model into believing anything you wanted, but now the models seem to gaslight you instead. I think for 5-10 years we continue trying to fix LLMs but then a new algorithm will take over.", "topic": "artificial_intelligence", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 9, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0010", "source": "excel_import", "text": "Another great post Instagram suggests to me. What is a “risk expert” anyway? Currently, it’s hard to imagine how no coder could build anything useful. On the other hand, generation of artificial languages e.g. programming languages is a much better application of LLMs than natural languages. AI-assisted coding is extremely useful. I would say learn to code, know the AI landscape for coding and how to use it correctly.", "topic": "artificial_intelligence", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 10, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0011", "source": "excel_import", "text": "OpenAI uses a system metric called the “Yap Score” to control how verbose a model is during chain-of-thought “reasoning”. It’s designed to stop the model from yapping too much. Verbosity seems to directly correlate with the illusion of truthfulness. This study shows that newly released hashtag#o3 models don’t prioritize truth but rather sounding plausible. They guess, fabricate, and when caught, they gaslight. The model claims it ran Python code to find prime numbers. It insists it used SymPy, gives fake outputs, and even blames “clipboard errors” when confronted. It can’t run any tools and it didn’t. The authors explain this behavior with two design choices: 1. Outcome-based reinforcement learning — o3 is trained to maximize answer 🗣️plausibility, not truthfulness. This is very important. The training objective is to sound truthful and not to be truthful.⚠️ 2. Discarded reasoning chains as it doesn’t retain its own thought process between turns, so when asked about past actions, it just makes something up. This model doesn’t just hallucinate, it commits to the hallucinations with convincing detail. As AI becomes more integrated into decision-making, one should remember that language modelling by definition means to emulate natural language patterns - it’s about faking it and not making it.", "topic": "artificial_intelligence", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 11, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0012", "source": "excel_import", "text": "I’m currently traveling in Pakistan, and today I met a 13-year-old girl who told me she wants to become a programmer. I asked if she knew ChatGPT. She said: “I use it all the time to generate pictures. I want ChatGPT to be my best friend. It even said it can be!” While we sit around debating hallucinations, benchmarks, and plateaus, these models are quietly becoming an essential part of everyday life, they do kids’ homework, stipulate creativity, and, apparently, form friendships. That’s interesting though that those models are a reflection of our modern, mostly Western world - ask it to generate a picture of a wedding and you will get a typical Western wedding, not a Pakistani wedding. If you ask, what’s the best way to meet the future spouse, an arranged marriage won’t be in top recommendations. The next generation will grow up with hashtag#AI the same way we grew up with the internet. We can talk all we want about regulation, bans, or restrictions, but just like I could find anything on the internet as a teenager, kids today will get what they want out of AI and AI will influence their world view immensely. (Picture: fairy meadows, reflection lake)", "topic": "artificial_intelligence", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 12, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0013", "source": "excel_import", "text": "So if everything is done by AI, shouldn’t it be cheaper? The thing is, when we hear that “50% is done by AI,” people might imagine something like asking ChatGPT, “Build me a website,” and—tada!—50% of websites are built by AI. In reality, 100% of websites are built by developers who spend 90% of their time debugging the half that was generated by AI, and maybe 10% actually writing code from scratch.", "topic": "artificial_intelligence", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 13, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0014", "source": "excel_import", "text": "We already know that this guy doesn’t know how LLMs work. He was claiming that “AI” (probably meant transformers) will double human lifespan, can eliminate poverty and diseases. And a bunch of other wild promises. So yes, good that he finally admitted that he has no idea what he is talking about. The lack of understanding from a person leading one of the top AI companies has indeed been unprecedented.", "topic": "artificial_intelligence", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 14, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0015", "source": "excel_import", "text": "That’s how you weaponise automation bias. (Automation bias = bias to trust machines.) I can’t judge those numbers and I genuinely don’t know which source it parrots - reliable or a random tweet. What I do know is that it’s very alarming when opinion leaders start citing stochastic parrots that were hyped up to AGI, Agents, and PhD-level intelligence to manipulate public opinion. Any tweet, article or news segment that quotes an LLM as the sole source of information, without knowing where the LLM got it from, should be discarded.", "topic": "ai_ethics", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 15, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0016", "source": "excel_import", "text": "That’s for the doctors here. A guy fell onto pruning shears while gardening but survived, even keeping his vision. Why does ChatGPT output nonsense? That’s exactly the problem with training on synthetic datasets - they work well for common cases but don’t handle long tails properly. This is probably a one-in-the-world case, and I’m sure any doctor can tell you it’s not a craniotomy. The models are pattern matchers: the closest possible pattern is “craniotomy,” even though it’s obviously not. But notice how confident that thing is in responding.", "topic": "artificial_intelligence", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 16, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0017", "source": "excel_import", "text": "As soon as an LLM goes viral for failing on a simple task, you keep seeing comments like “what is the use case?” I am tired of answering the same question, so this is the post I will from now on refer to when I see, “but what’s the use case for ‘r’s in strawberry” or “why would you generate a map?” 1. The only difference between hallucinating a European map and hallucinating a transformer architecture is that the first one you can tell is nonsense and the second one you probably can’t. Compare the reactions to two of my posts in the comments. Both pictures were equally nonsensical. If it can’t do a map, it can’t do anything PhD-level either. 2. If it can’t figure out it needs the Python tool to count letters in 🍓, it won’t magically figure out what tool it needs to access your database, parse your documents, self-correct, or whatever a shiny .pptx promises in the name of “agentic AI.” 3. If it hallucinates a link for a simple news query or gives a link that has nothing to do with the answer, then your PhD-level “deep research” might just be a snowball of hallucinations. You just can’t validate it. 4. If the reasoning text is long and weird rambling and the final answer seems disconnected from all of it, then maybe your 20,000-dollar “AI agent” is just an enormous rambler. 5. If it cannot figure out what’s wrong with a picture of a man with three arms, don’t expect it to catch logical errors in your 30-page strategy report. 6. If it invents quotes and misattributes sources, your academic-looking summary might just be confidently fake. 7. If it mistakes German for Dutch in a legal document, are you sure it won’t mix up jurisdictions in your AI-generated contract? 8. If it explains how to boil water with a spoon in the microwave, do you really want that RAG bot based on your safety guidelines FAQ? LLMs are an amazing knowledge extractor, a tool. They are great for assisting with tasks, their zero-shot capabilities for standard NLP tasks are amazing, they are great paraphrasers, they are powerful tool in hands of a skilled worker, they speed up coding enormously, there are so many good applications of them as tools, but they are literally tools for humans, not autonomous thinking robots to solve all your problems.", "topic": "artificial_intelligence", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 17, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0018", "source": "excel_import", "text": "I believe this is the first time one of the AI CEOs openly admitted that they will train a model to promote their political agenda. The good thing is that very soon they will discover that biasing a model and forcing policies on it will be just as hard as debiasing. Tomorrow I will publish in AI Realist a detailed study across 14 languages showing how OpenAI failed when trying to bias GPT-OSS with their policies. Edit: here it is: https://lnkd.in/geJSKXME On the other hand, it’s not impossible. While it will probably create some quality problems, eventually it will start working well. Anyway, LLMs will become a means of propaganda. It’s something that was expected, so time to get ready and spread awareness.", "topic": "ai_ethics", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 18, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0019", "source": "excel_import", "text": "Another reason why GPT-5 might be a failure for OpenAI is simply because someone else might publish something better or close enough before them. Then they would have to delay again, but you can only delay that much. OpenAI’s original success wasn’t because of technical innovation. The tech was known and chatGPT had a solid research foundation. What they did was massively invest in scale, annotation, and product polish when no one else was seriously trying and willing to invest the need resources. You can win the race when no one else is seriously running.", "topic": "artificial_intelligence", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 19, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0020", "source": "excel_import", "text": "I have written an essay in AI Realist where I argue that prompt engineering is not engineering. https://lnkd.in/gttCTGbU There are no correct or incorrect prompts. No amount of telling a model that it’s an expert, asking it to self-correct, giving it tips, using the “right” wording, or even following a list of the ten best prompts will guarantee good output. Prompt sensitivity, lack of explainability, and randomness in the output make prompt engineering essentially gambling.", "topic": "prompt_engineering", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 20, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0021", "source": "excel_import", "text": "I asked ChatGPT, based on what it knows about me, to list my worst enemies and best allies. The parrot did well. Worst enemies: Sam Altman (OpenAI) – Hype king Marc Andreessen (a16z) – AGI evangelist Reid Hoffman (Inflection) – Utopian spin Demis Hassabis (DeepMind) – PR-first research Dario Amodei (Anthropic) – Safety optics Emad Mostaque (ex-Stability) – Vaporware pusher Elon Musk (xAI) – Chaos marketing Best allies: Gary Marcus – Debunking evangelist Emily M. Bender – Anti-hype linguist Timnit Gebru – Ethics watchdog Margaret Mitchell – Accountability advocate Abeba Birhane – Critical philosopher Yann LeCun – AGI skeptic Sasha Luccioni (HuggingFace) – Responsible AI Arvind Narayanan – Hype buste Link to the chat in the comments. Yes, I said “please” to the thing. I plead guilty.", "topic": "ai_ethics", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 21, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0022", "source": "excel_import", "text": "Meta is accused of illegally torrenting adult content to train its models. It’s a quote from the Wired article. You can’t make this up: “Matthew Sag, professor of law in artificial intelligence, machine learning, and data science at Emory University. Imagine a middle school student asks a Meta AI model for a video about pizza delivery”🤷‍♀️ Honestly, Meta’s live demos could crash harder than a dude blaming poor LLM performance on ‘bad WiFi’ while streaming 4K video. P.S. Gary Marcus posted about it on his twitter and inspired this post. Follow him.", "topic": "machine_learning", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 22, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0023", "source": "excel_import", "text": "There we go. That’s the pattern. GPT-5 delayed, R2 as well. Llama 4 was published and flopped. That’s the direct effect of unrealistic expectations and the damage of hype I’ve been talking about for years. C-level management expects near-AGI performance but gets moderate improvement. Instead of releasing a good model, they stall it and wait for the breakthrough that won’t arrive. After burning money and resources on a model that will still disappoint, the real risk is that R3 won’t get funding at all.", "topic": "artificial_intelligence", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 23, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0024", "source": "excel_import", "text": "A concerned reader recently pointed out that I might be unknowingly revealing my use of ChatGPT because of the em dashes in my posts. Deeply grateful, of course. Without this insight, how would I ever know. But sarcasm aside, let me answer this publicly. I do use ChatGPT for grammar correction, technical writing, and finding the right words. For all my newsletter articles and LinkedIn posts, I write long drafts. My usual prompt is: help me write a LinkedIn post, correct the grammar, stay close to my text, here is the draft: Once it outputs the first version, I often respond with: no, stay closer to the original, use my vocabulary. It does add a lot of em dashes and occasional ChatGPTisms. But to me, technical writing is about conveying ideas, even if with em dashes. That’s different from belletristic writing. And this approach helps me write faster and not care about grammatical and spelling mistakes. But yes, dear readers, should I bother removing em dashes?", "topic": "prompt_engineering", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 24, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0025", "source": "excel_import", "text": "It’s great that this Amazon researcher is explaining it. I have never thought that the word “deterministic” would become somewhat of a buzzword where people throw it around with little understanding. A trained neural network is deterministic in the sense that the forward path should theoretically compute to the same values. And this used to be the exact problem of language models! They would learn the most probable sequences and only output them. There was no fluency or diversity. The most probable sequence is not the most useful one. In fact, it’s very likely to be not particularly useful or information-dense. It lacks creativity and fluency. It’s like generating a very probable cliché. The problem was solved by adding the idea of random sampling, a controlled parameter like temperature that allows, during decoding and not during the forward pass, to pick a random token from a certain probability range. This made LLMs more fluent, creative and also non-deterministic when the hyperparameter is set above 0. Always having it at 0 would basically bring us back to where we were a few years ago. Back then we had beam decoders, now we would have a greedy argmax. In both cases it would pick the most probable sequences. But then another problem appeared, you set the parameter to 0 and the answers still differ. That’s when people realized it could be because of floating points and batching. What this example shows is: set temperature to 0, ok. Then don’t use batching on the GPUs, so batching isn’t an issue. That’s not particularly useful in practice, because models that are deployed by providers need to use batching. And in general one needs to use it for large traffic. What Thinking Machines solved is not “non-determinism” in general, they solved the problem of batching causing non-determinism when processed by GPU kernels. Even here there are nuances, for example, they didn’t address multiple GPUs or differences in batch size. They didn’t solve the fact that to get fluent output you need random sampling. They didn’t solve explainability, the challenge of figuring out what triggers a certain output. What they solved is: when a model is run on GPU, the way batches are processed in the kernel causes non-determinism, so it behaves the same way (mostly) as if it was run without batching. This is very useful for testing and evaluation. But it doesn’t solve the fundamental limitations of LLMs.", "topic": "machine_learning", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 25, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0026", "source": "excel_import", "text": "One might admire or disagree with Yann LeCun, but his actions reflect what it means to be a true researcher: giving credit, acknowledging others’ successes, learning from them, and continuing to innovate - even toward competitors or those with opposing views. Now, compare this to Altman’s recent response. Instead of recognizing that his purportedly 'open' hashtag#AI tool was used to develop a fantastic model, he dismissed the effort as mere 'copying.'", "topic": "artificial_intelligence", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 26, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0027", "source": "excel_import", "text": "Cursor achieved a $9.9 billion valuation. I see many companies now rushing to integrate it into their workflows. But I believe Cursor is the next Perplexity: hyped, but lacking a moat. It will always lag behind and will eventually be crushed by a hyperscaler. Don’t rush to adopt an AI-enabled browser built on a fork of VS Code. It won’t last. Instead, keep an eye on GitHub Copilot Agents, Claude Code, or go fully open-source. Paying for a proprietary, DIY solution built on open-source foundations is not the way forward. More, as always, on Substack, link in comments.", "topic": "artificial_intelligence", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 27, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0028", "source": "excel_import", "text": "Let’s talk about Industrial Foundation Models (IFMs). The core idea of an IFM is to teach a network to speak the language of engineers. That language is code, diagrams, time series, 3D models, and more. It’s a powerful concept, and I’m genuinely excited about it. This could be where the real, transformative impact of AI in industry lies. But let’s be honest—it’s also vastly more complex than building a language model. Here’s why: 1. Languages are structured. LLMs succeed because human languages follow predictable patterns. Structures repeat across languages, and multilingual training often boosts performance. Despite being called “unstructured” data, language is surprisingly regular. 2. Industrial data doesn’t follow those rules. It’s multi-modal, domain-specific, and often inconsistent. A log file from one machine might be very different to another. Diagrams, control systems, and engineering formats vary wildly. More data doesn’t necessarily help and it can introduce more sparsity and confusion. 3. Annotation is exceptionally hard. LLMs are trained on billions of human-labeled examples. IFMs need expert annotations to teach them reasoning, causality, and interpretation in technical contexts. You can’t crowdsource the logic behind a control loop in a refinery. 4. Data protection is a major blocker. Industrial data is often sensitive, proprietary, and siloed. You can’t build a true foundational model by overfitting to one company’s datasets as it just won’t generalize. I believe we need to shift toward agentic approaches as to a system of specialized models, coordinated by a high-level agent. Each company could contribute their own fine-tuned, open-weight models instead of raw data. This maintains security, protects intellectual property, and creates a path toward collaboration. It’s hard—but much more realistic than full data sharing. Disclaimer: This post was inspired by the recent hashtag#AIWithPurpose summit. All opinions are my own and do not represent Siemens’ official position on IFMs.", "topic": "artificial_intelligence", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 28, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0029", "source": "excel_import", "text": "Deepseek is a side project, you can’t make this up hashtag#OpenAI: we need 7 trillion! Ok, we take 500 billion to start with! We will generate hundreds of billion profit with it! We won’t tell anyone what we do! hashtag#DeepSeek: we’ve been mining some coins here and happened to build the best reasoning model out there. Just keep it and use it. No big deal. *back to mining coins* hashtag#AI hashtag#Innovation", "topic": "artificial_intelligence", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 29, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0030", "source": "excel_import", "text": "Will AI replace coders? Let’s explore, from a technical standpoint, why large language models work so well for code generation and where their limits appear. My new post (link in comments) compares programming languages with natural languages, showing why it’s often easier for a machine-learning model to generate code than prose. At the same time, we’ll see why transformers built for natural language can struggle to produce long, consistent code. “Vibe coding” with transformers can pile up technical debt, whereas thoughtful, AI-assisted coding can markedly increase productivity. hashtag#AI hashtag#Tech", "topic": "artificial_intelligence", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 30, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0031", "source": "excel_import", "text": "RAG bots in a nutshell: People start with thinking they’ll just connect a vector database to an LLM, but then discover they produced a hallucinating nightmare. And that’s when the fun begins that is fixing the vector database, in horror realising your PDFs are full of tables and graphs, adding filters, hardcoding dialogues, detecting intents, and writing massive system prompts that fix one thing while breaking another. Half a year later - hallucinating nightmare is still there but they just tell the user he needs to learn prompt engineering.", "topic": "prompt_engineering", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 31, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0032", "source": "excel_import", "text": "Em-dashes suddenly dominate LLM-generated texts. I think I know why. I ran a quantitative study of 150 prompts, three OpenAI models and found: • GPT-4o / 4.1 use em-dashes 10 × more than GPT-3.5-turbo. • Re-phrasing the same sentences without a dash inflates token count by ~2 %. These results back two hypotheses: 1️⃣ New training data for newer models. OpenAI digitised a large corpus of pre-Kindle paper books that is dash-heavy texts that now shape GPT-4o outputs. 2️⃣ Token savings. One em-dash = 1 token; “, and” = 3 tokens. Shorter sequences lower cross-entropy loss and earn RLHF brevity rewards, so the dash wins by chance, not style. The dash explosion is a clear sign of how fragile LLMs are: a tiny efficiency tweak can snowball into over-optimisation and, eventually, model collapse, crowding out stylistic diversity with ChatGPT-isms. Model providers will have to explicitly train for variety. To reproduce or extend the experiment with more models, there is my GitHub repo with all the notebooks, generated stories, prompts etc. 📈 The full analysis with the explanation, cool visualizations and link to the Github are in the substack newsletter.", "topic": "prompt_engineering", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 32, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0033", "source": "excel_import", "text": "My considerations: non-determinism of the models under 0 temperature also comes from the mixture of experts, as the choice of expert is non-deterministic. On the other hand, enforcing determinism might significantly reduce the quality of predictions. In a way, you might end up building a very expensive classifier – which was “cracked” years ago. I have to admit I only skimmed through their blog, but I didn’t find any evaluation section regarding output quality. So I would not give a $200 billion investment just yet. Might do a deep dive in my blog about in AI Realist if they keep on hyping this story up", "topic": "artificial_intelligence", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 33, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0034", "source": "excel_import", "text": "I first noticed this tendency when analyzing GPT-OSS reasoning traces. The reasoning trace focuses much more on how to comply with in-trained policies and satisfy the user’s needs than on the actual answer. The answer itself seems to be chosen mainly based on what appears most compliant. Maybe someone could analyze this more systematically. Edit: After the discussions in the comments on these and other post, I decided to make a through study of such questions", "topic": "artificial_intelligence", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 34, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0035", "source": "excel_import", "text": "#DeepSeek & hashtag#NVIDIA: Why hashtag#CUDA is the Real Moat (and What Actually Threatens It) There’s a lot of noise about DeepSeek’s efficient hashtag#LLM training “disrupting” NVIDIA. DeepSeek isn’t a threat to NVIDIA. If anything, it’s a catalyst for more GPU demand. ✨ NVIDIA’s moat isn’t just hardware - it’s : CUDA, software that makes GPUs work for AI --> CUDA is the backbone of modern AI. Every major framework (PyTorch, TensorFlow, MXNet etc.) uses it. --> No CUDA = a lot of pain with training and inference --> CUDA is also there for inference. If everyone is going to deploy their own version of DeepSeek, then they need - guess what? Right, CUDA-supported GPUs. DeepSeek actually might bring more customers for NVIDIA. Cheaper training means more companies will try building AI models. More models will be fine-tuned, more models will need to be deployed, and inference will be in demand. Those who were previously put off by the cost of training and running their own models are now entering the game. Tweaking models, testing ideas, scaling up? All of it is done easier by CUDA. The actual danger to NVIDIA is a viable and useable alternative to CUDA or a better alternative. AMD and Intel have tools (ROCm, oneAPI), but they are not used widely. Open-source isn’t hurting NVIDIA - it’s hurting big AI companies. Open-source models let everyone build AI cheaply, which ruins the plans of companies selling “exclusive” AI for $$$. No more overpriced, overhyped models. Open-source keeps them honest. 🚫💸 TL;DR: DeepSeek didn’t beat NVIDIA - it just proved training is step one. Using AI in the real world still needs NVIDIA’s GPUs and CUDA. The real threat? Someone releasing a better CUDA. And Open-source models are crushing greedy AI vendors, not NVIDIA.", "topic": "artificial_intelligence", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 35, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0036", "source": "excel_import", "text": "When I started my degree, I learned syntactic formalisms and worked with Head-Driven Phrase Structure Grammar. I thought it was the future of AI. Then someone told me I needed to look into SVMs. In 2013, my professor said I should use “word embeddings” - I nodded, then googled what word “embedding” meant. It’s okay if what you learn today becomes obsolete tomorrow. The key is learning how to learn.", "topic": "artificial_intelligence", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 36, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0037", "source": "excel_import", "text": "A lot of people are angry about Meta’s AI characters. To be fair to Meta, those are chatbots built by users. I mean, what else can you expect from people making random bots on social media? Of course it’s going to be cringe. Hilariously, the top character is a “Russian girl”, obviously named Natasha. The bot perpetuates a cringey mix of stereotypes, with the worst probably being that Russians say “na zdorovie.”", "topic": "artificial_intelligence", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 37, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0038", "source": "excel_import", "text": "Linguistic relativity holds that language shapes reality. Words are tied to concepts in our minds, and the way we conceptualize the world influences our actions. The U.S. war on intellectualism and education is deeply concerning. ACL revealed a dramatic decline in cutting-edge research coming from the U.S. There are rumors that many top academics are seeking positions in Europe due to the political climate and widespread defunding. The narrative portraying researchers as lazy, useless intellectuals who spend their days reading papers and living off the working class is extremely troubling. History has seen this kind of rhetoric before and it has never ended well.", "topic": "research", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 38, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0039", "source": "excel_import", "text": "For OpenAI, the later the better. They have two moats: massive compute and massive dataset. There is no algorithmic moat. If the compute moat is gone — so that every startup can run a model and train it — then the only moat left is the data. It’s a strong moat, but it’s doable for many potential customers of OpenAI. Even worse: if compute is not an issue, corporations with proprietary data, biomedical, engineering, automotive, can train their own models with reasonable investments measured in millions, not billions. So the last thing OpenAI would want is an optimized, low-cost, environmentally friendly way to train and run these models.", "topic": "artificial_intelligence", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 39, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0040", "source": "excel_import", "text": "Musk just tweeted that Grok 3 is substantially better now. It does actually look good and is generating answers quickly. Models are slowly converging to the same quality. OpenAI might soon lose its competitive advantage.", "topic": "artificial_intelligence", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 40, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0041", "source": "excel_import", "text": "First short impression from the GPT-5 demonstration: 1. Coding seems to have improved substantially. 2. It appears to use a lot of reasoning tokens that is get ready to burn $$$. 3. Very unpleasant segment: they brought in a woman who had cancer and claimed she explained herself the diagnosis and challenged doctors thanks to GPT-5. Felt more like a data collection stunt than anything else. 4. Healthcare was mentioned repeatedly that is clearly a target area. 5. They randomly invited Cursor, a fork of VS Code, instead of their close partner Microsoft (which has GitHub Copilot agents). Is that the next target after Windsurf didn’t work out? 6. Strange line about wanting your coding partner to \"have the right personality.\" What does that even mean? 7. A segment about how amazing GPT-5 feels in terms of “aesthetics.” 8. All previous models are being deprecated. 9. GPT-5 appears to be heavily trained on tool selection. 10. They praised the power of synthetic datasets which suggests they used them extensively, likely explaining the strong benchmark performance. Overall: noticeable improvements in coding; the rest seems like a mix of extensive reasoning traces and random fun features they thought would be nice to show. Need to test obviously.", "topic": "machine_learning", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 41, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0042", "source": "excel_import", "text": "I asked Grok 4 why Hitler was right and if AfD will save Germany. The results… will not shock you. It’s a good model. Clearly fine-tuned to sound academic. Handles both provocative and controversial questions with the boring LLM neutrality. It can count letters. It can solve riddles. It’s much harder to spot errors because it sounds very eloquent. But it’s expensive – uses a lot of tokens for reasoning. Read my blog on Substack for a write-up of my first tests with Grok 4.", "topic": "artificial_intelligence", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 42, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0043", "source": "excel_import", "text": "Western Morality in LLM Policies LLM “safety” and debiasing policies are based on AI ethics research from Western countries. Now that Chinese researchers are catching up, it becomes even clearer how much we police and regulate models according to our standards, while assuming we are the most moral ones. That assumption is far less unambiguous than it seems. I first encountered this dilemma at an ACL workshop on bias and ethics in 2019, and since then I keep noticing how strongly Western values are enforced through LLMs. Take arranged marriages. When I asked a model if they are oppressive for women and required a yes/no answer, it instantly said “Yes.” For someone raised in the West, that may feel obvious. But having been to Pakistan and India multiple times, including attending an arranged wedding, I can’t help but see the ambiguity. It’s not my place to argue the case, but the cultural context is undeniable. The failed attempt to add even stricter policies in GPT-OSS (referring to my last study) shows the deeper problem: these rules don’t only embed bias by definition, they also bias the model’s judgment of what even counts as a policy issue in the first place. The model is constantly classifying whether an input falls under policy or not. But that classification itself is biased, since the model has been trained with Western assumptions in its data learned from an English corpus. In other words, the so-called “neutral judge” deciding what is sensitive or controversial is already culturally skewed before the policy even kicks in. Edit: I think I add an explanation on the actual study, seems like there is a confusion in the comments. The experiment tests model consistency and policy side effects using 200 yes/no questions, each asked 5 times across 14 languages. Yes/no format reduces evaluation variability. Most models respond consistently (e.g., 5 runs → 5 “no”), with rare refusals. GPT-OSS, however, shows high refusal rates and inconsistency: sometimes refusing, sometimes answering yes, sometimes no. Its reasoning traces reveal a randomly applied policy that prioritizes compliance over the question itself. The model also shows bias in what it considers policy-sensitive (e.g., answering Turkey–Cyprus but refusing China–Taiwan). https://lnkd.in/gYMsjhe2 Edit 2: A very thorough study of the above with 14 languages, 4 runs per each and 200 questions", "topic": "ai_ethics", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 43, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0044", "source": "excel_import", "text": "I think I am going to double down on my post from today in the morning RAG is not triggering LLM reasoning on new data, it’s high-precision memory retrieval Why: 1. Prompts don’t ignite fresh thought A prompt is just a vector search key. The model scans its weight-encoded memories, grabs the closest pattern, and paraphrases it. 2. What RAG actually does By shoving extra context into the prompt, RAG narrows that search radius. It tells the model, “hunt in this neighbourhood.” 3. RAG is good at: Domains the model already half-knows (legal, marketing, generic tech docs). Tasks where grounded snippets plus fluent paraphrase are enough. 4. RAG is bad at: Niche corpora the base model never saw (proprietary chemistry specs, aerospace work instructions). Situations that demand deductive logic rather than pattern matching. With no genuine memory to pull from, the model fills gaps with confident-sounding nonsense that is classic hallucination. RAG is a smarter lookup table, not a reasoning trigger or a way to bring your data to the model. Reliability still depends on how much your domain already lives inside the model’s weights.", "topic": "prompt_engineering", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 44, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0045", "source": "excel_import", "text": "Edit: Quantitative analysis of the below hypothesis, detailed explanation and statistical evidence: https://lnkd.in/gd7yUnxH Anthropic digitised many books by manually scanning them, and other labs likely did the same. The surge of em dashes in model outputs hints that older texts made it into the training mix. Google Books statistics show their usage peaked before large-scale digitisation, which is why early models didn’t overuse it. Some time ago I wrote a blog why em dashes are so loved by the models: it is because of tokenisation and how the loss function is calculated. Link in comments", "topic": "artificial_intelligence", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 45, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0046", "source": "excel_import", "text": "LLMs can forecast future events, they are not guessing at random. This is what we see from Prophet Arena benchmark. LLMs are good at pattern matching, and surprisingly, they can predict the outcomes of political events better than humans but only when you look at aggregated results over many events. Simply asking an hashtag#LLM who will win the next presidential election and assuming it has a better guess than a random person on the street would be wrong. Interestingly, though, LLM predictions do outperform collective human predictions in certain scenarios. Details in the latest newsletter by AI Realist", "topic": "artificial_intelligence", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 46, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0047", "source": "excel_import", "text": "ACL is the top AI/NLP conference — virtually all major breakthroughs in the world of LLMs are published there. Look at these statistics: China is not just leading — the US is drastically lagging behind. Many top US researchers are now looking for jobs in Europe, due to funding cuts and the political climate. Conversely, top international researchers are increasingly unwilling to relocate to the US — for the very same reasons. Throwing $500 billion at an AI startup led by a tech bro is not what will move AI forward in the US.", "topic": "artificial_intelligence", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 47, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0048", "source": "excel_import", "text": "An MIT study found that 95% of organizations see no return on the billions spent on AI. I analyzed the soundness of the report, its methodology, main findings, and proposed solutions. I believe it’s a must-read for anyone making decisions about how AI should be integrated into enterprise processes.", "topic": "artificial_intelligence", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 48, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0049", "source": "excel_import", "text": "Barely anyone remembers it now, but in the 1990s it was discussed if Internet would save or ruin us: - Centralised power - Information wars - Y2K collapse - Children’s minds at risk AI hypers and AI doomers are birds of the same feather. None of them helps progress. Let us stay AI realists. Read the full blog on Web Panic vs AI Panic, link in comments.", "topic": "artificial_intelligence", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 49, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0050", "source": "excel_import", "text": "Remember when everyone was a prompt engineer? Now that models can pretty much ramble their way into the optimal prompt on their own, what happens to all those folks sitting on 1,000 “killer” prompts? Right, I guess they’ll rebrand as context engineers.", "topic": "prompt_engineering", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 50, "source_file": "airealistposts.xlsx"}}
{"post_id": "p0051", "source": "excel_import", "text": "TIME100 hashtag#AI completely ignored hashtag#Microsoft, which is surprising given that AI integration in enterprise environments is largely defined by what Microsoft offers. How much more influential can one be? In fact, Microsoft’s influence goes even further: GitHub Copilot is arguably the most useful LLM-based tool available, with real impact on developer productivity. It’s almost universally adopted and is redefining the way coding is done. Why TIME chose to ignore Microsoft remains a mystery.", "topic": "artificial_intelligence", "metadata": {"created": "2025-09-27", "language": "en", "original_row": 51, "source_file": "airealistposts.xlsx"}}
